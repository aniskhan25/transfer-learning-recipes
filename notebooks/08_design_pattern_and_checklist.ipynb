{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 08 Design pattern and final checklist\n\nSections covered: 10-12 (design pattern, checklist, 3 key points).\n\nThis notebook operationalizes a recommendation engine from measured stability signals."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import subprocess\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    ROOT = Path.cwd().resolve()\n",
    "LOGS = ROOT / 'outputs' / 'logs'\n",
    "FIGS = ROOT / 'outputs' / 'figures'\n",
    "LOGS.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_config(config_path: Path, use_progress: bool = True):\n",
    "    cmd = ['python', str(ROOT / 'scripts' / 'run_transfer.py'), '--config', str(config_path)]\n",
    "    if use_progress:\n",
    "        cmd.append('--use-progress')\n",
    "    subprocess.run(cmd, cwd=ROOT, check=True)\n",
    "\n",
    "def read_method(name: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(LOGS / f'transfer_{name}.csv')\n",
    "\n",
    "def deep_update(base: dict, patch: dict) -> dict:\n",
    "    for k, v in patch.items():\n",
    "        if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "            deep_update(base[k], v)\n",
    "        else:\n",
    "            base[k] = v\n",
    "    return base\n",
    "\n",
    "def apply_fast_dev_profile(cfg: dict) -> dict:\n",
    "    cfg = deepcopy(cfg)\n",
    "    data = cfg.setdefault('data', {})\n",
    "    train = cfg.setdefault('train', {})\n",
    "\n",
    "    if 'source_train_per_class' in data:\n",
    "        data['source_train_per_class'] = min(int(data['source_train_per_class']), 220)\n",
    "    if 'source_test_per_class' in data:\n",
    "        data['source_test_per_class'] = min(int(data['source_test_per_class']), 80)\n",
    "    if 'target_train_per_class' in data:\n",
    "        data['target_train_per_class'] = min(int(data['target_train_per_class']), 30)\n",
    "    if 'target_test_per_class' in data:\n",
    "        data['target_test_per_class'] = min(int(data['target_test_per_class']), 80)\n",
    "    if 'probe_per_class' in data:\n",
    "        data['probe_per_class'] = min(int(data['probe_per_class']), 40)\n",
    "\n",
    "    data['batch_size'] = min(int(data.get('batch_size', 128)), 64)\n",
    "    data['num_workers'] = 0\n",
    "\n",
    "    if 'source_epochs' in train:\n",
    "        train['source_epochs'] = min(int(train['source_epochs']), 2)\n",
    "    if 'target_epochs' in train:\n",
    "        train['target_epochs'] = min(int(train['target_epochs']), 3)\n",
    "\n",
    "    train['gradual_schedule'] = {\n",
    "        '1': ['backbone.layer4'],\n",
    "        '2': ['backbone.layer3'],\n",
    "    }\n",
    "    return cfg\n",
    "\n",
    "def make_profiled_config(base_name: str, notebook_tag: str, fast_dev_run: bool, overrides: dict | None = None) -> Path:\n",
    "    cfg = yaml.safe_load((ROOT / 'configs' / base_name).read_text())\n",
    "    if fast_dev_run:\n",
    "        cfg = apply_fast_dev_profile(cfg)\n",
    "    if overrides:\n",
    "        cfg = deep_update(cfg, deepcopy(overrides))\n",
    "\n",
    "    suffix = 'fast' if fast_dev_run else 'full'\n",
    "    out = ROOT / 'configs' / f'tmp_{notebook_tag}_{suffix}.yaml'\n",
    "    out.write_text(yaml.safe_dump(cfg, sort_keys=False))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FAST_DEV_RUN = False\ncfg_path = make_profiled_config(\n    base_name='transfer_core_related.yaml',\n    notebook_tag='08_checklist',\n    fast_dev_run=FAST_DEV_RUN,\n)\nrun_config(cfg_path, use_progress=not FAST_DEV_RUN)\nmethod_frames = {m: read_method(m) for m in ['scratch', 'feature_extraction', 'gradual_unfreeze', 'naive_finetune']}\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "table = []\nfor method, df in method_frames.items():\n    final = df.iloc[-1]\n    table.append({\n        'method': method,\n        'final_target_acc': float(final['target_test_acc']),\n        'final_feature_drift': float(final['feature_drift']),\n        'final_retention': float(final['source_retention_acc']) if 'source_retention_acc' in df else float('nan'),\n        'final_grad_norm': float(final['grad_norm']),\n    })\nsummary = pd.DataFrame(table).sort_values('final_target_acc', ascending=False)\nsummary\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def recommend(df: pd.DataFrame) -> str:\n    best = df.sort_values('final_target_acc', ascending=False).iloc[0]\n    low_drift = df[df['final_feature_drift'] < 0.25]\n    if not low_drift.empty:\n        stable_best = low_drift.sort_values('final_target_acc', ascending=False).iloc[0]\n        return f\"Prefer {stable_best['method']} for production (best stable accuracy).\"\n    return f\"Prefer {best['method']} for raw accuracy; add stabilizers before production.\"\n\nrecommendation = recommend(summary)\nrecommendation\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Final checklist mapping\n\n- Are tasks related: compare related vs unrelated curves.\n- Is transfer stable: inspect drift, retention, grad norm.\n- Is it worth it: verify improvements over scratch on both final score and convergence speed."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
